{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693d848f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import json\n",
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "# from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "# NLTK Stop words\n",
    "\n",
    "# Remove stop words \n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18179ce8-101f-4f67-8411-4e206bad8b3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7e69b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Abstract\n",
      "0  Designing and deploying artificial intelligenc...\n",
      "1  Agriculture faces increasing challenges all ar...\n",
      "2  Sustained intensification in agricultural prod...\n",
      "3  With support from the Centers of Research Exce...\n",
      "4  This award is funded in whole or in part under...\n"
     ]
    }
   ],
   "source": [
    "#Import data from excel\n",
    "df = pd.read_excel('Abstracts.xlsx')\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e778f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       Designing and deploying artificial intelligence (AI) tools in agriculture represents an exciting opportunity....      ]
    }
   ],
   "source": [
    "# Merge all columns into a single series\n",
    "merged_series = df.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Convert the series to a single string\n",
    "combined_text = ' '.join(merged_series)\n",
    "\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06366632",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       "Designing and deploying artificial intelligence (AI) tools in agriculture represents an exciting opportunity ...     ]
    }
   ],
   "source": [
    "#OPTIONAL: convert to string if you want to clean or manipulate any further\n",
    "data = str(combined_text)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b42c83a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\...",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#This cell sets up our stop words list for terms that do not have much meaning in text analysis\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_add= ['from', 'subject', 're', 'edu', 'use','also', 'well', 'including', 'simple', 'yet',  'work', 'inform',  'generally','given', 'work', 'help', 'play', \n",
    "                'et', 'al','across', 'towards','allow', 'body','affect','give', 'addtion','know', 'whose','real','open','often','although','act','find','even',\n",
    "              'suggest','connect','tightly','release','space','function','regulate','control','role','apt','process', 'finely', 'tune','regulation','critical','functioning',\n",
    "              'endeavor','finely','conditions','term','project','using','changes','fundamental', 'build','block','make','addition', 'provide', 'train','individual',\n",
    "              'feasible', 'complex','mechanisms', 'remain', 'unclear', 'overarch','system', 'thereby', 'movement', 'valuable', 'educational','consider','waste',\n",
    "              'study', 'field', 'establish','award','understand','expression','community','reach', 'the', 'on', 'in',\n",
    "              'statutory','statutory','head','call','group','new','size','behavior','leave','internalize',\n",
    "              'via','ecosystem','model','organisms','harsh','concurrently','precise','small','essential','long','release',\n",
    "               'random','live','range','really','form','global','potential','collection','important','exposure','extreame','order', 'recipient','extreme','break','still','together',\n",
    "              'cells','build','block', 'live','however','dont', 'exactly', 'cells', 'form', 'first', 'place', 'dont','molecules','cell', 'finally','question', 'challenge', 'pose', 'require',\n",
    "               'significant','edit', 'hold','regulator','put','cant','decline', 'interact','attendance','major', 'systems','alter','host','many','may','must','support',\n",
    "              'think','time','defend','wonder','adjustable','operations', 'meet','lack','conduct','whether','trait','directly','effect'\n",
    "               ,'enhance','consumer', 'demand', 'value','address', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "                'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "                'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "                 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "                 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "                'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "                'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "                'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "                's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    "                 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "                'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "                 \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"pattern\", \"practices\", \n",
    "               'lesson', 'instructor', \"nsf\", \"represent\", \"technolog\", \"collaboration\", \"artificial\", \"intelligence\", \n",
    "               \"agricultural\", \"agriculture\", \"ai\"]\n",
    "\n",
    "\n",
    "stop_words.update(stopwords_add)\n",
    "# stopwords.extend(stopwords_add)\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0766bae0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       "designing deploying tools represents exciting opportunity international uniting...     ]
    }
   ],
   "source": [
    "#This function ensures that the text is a string, removes punctuation and converts the text to lower case. \n",
    "#Then the text is split and stop words are filtered out. \n",
    "#You can also tokenize the text in this step if preferred.\n",
    "def remove_stopwords(combined_text):\n",
    "    import string\n",
    "    # nltk.download('punkt')  # Download tokenizer if not already done\n",
    "    text = combined_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    tokens = text.split()\n",
    "    # tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "filtered_text = remove_stopwords(combined_text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811c1324",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       "['designing', 'deploying', 'tools'...     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Tokenize here if you did not tokenize in the previous function remove_stopwords. \n",
    "#The WhitespaceTokenizer just splits the text based on spaces, while word tokenizer splits text by each word.\n",
    "#Either one is fine to use based on how specific you want to be to split the text.\n",
    "tokenizer_w = WhitespaceTokenizer()\n",
    "tokenized_text = tokenizer_w.tokenize(filtered_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8417e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing text turns the text into a list\n",
    "type(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6c4537",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
       "['[designing, deploying, tools, represents, exciting, ...    ]
    }
   ],
   "source": [
    "#I do not prefer distracting quotes in the tokenized text, so this cell removes the extra punctuation.\n",
    "token_clean_text = [re.sub(\"\\'\", \"\", str(tokenized_text)) for sent in tokenized_text]\n",
    "\n",
    "print(token_clean_text[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb4c00d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Since tokenizing the text turned it into a list as we checked earlier, we are converting back to a string.\n",
    "token_clean_strg = str(token_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ac4813",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6520"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1dac19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sahluwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Now we still need to stem the words which means that the words will be deduced to their base terms. \n",
    "#Stemming preprocesses the text for LDA analysis.\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download ('punkt_tab')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", True)\n",
    "words = word_tokenize(token_clean_strg)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5516f568",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e72017",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stemmed_words_strg = str(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "623bf576",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651556645"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stemmed_words_strg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a4b621",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#We are merely cleaning the text a little more and the next 2 cells are just to view the text, optional\n",
    "cleaned_texts = [sent.replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\") for sent in stemmed_words_strg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a2fc87d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully as 'output.txt'\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "     for item in cleaned_texts:         \n",
    "         f.write(item)\n",
    "print(\"File saved successfully as 'output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "861bfc8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ \"\" [ design  deploy  tool  repres  excit  opportun  intern  unit  divers  expertis  resourc  tackl  challeng  aim  impact  develop  deploy  democrat  tool  farmer  manag  pest  stressor  effect  make  farm  less  riski  profit  sustain  plan  creat  aidriven  tool  person  manag  advic  crop  yield  sustain  farm  initi  bring  scientist  practition  us  india  japan  foster  intern  innov  aidriven  approach  benefit  mediums  farmer  offer  easytous  access  technolog  pursu  climatesmart  includ  compon  multilater  engag  inspir  next  generat  expert  eager  seek  pursu  multilater  research  partnership  us  india  japan  develop  deploy  aidriven  tool  product  team  two  area  collabor  effort  develop  hybrid  machin  learn  model  combin  sensor  proxim  remot  data  biophys  knowledg  yield  stress  predict  ii  util  agronom  data  biotic  insect  weed  diseas  abiot  nutrient  defici  herbicid  injuri  finetun  deploy  larg  vision  languag  model  develop  aiira  one \n"
     ]
    }
   ],
   "source": [
    "#optional if you want to see the cleaned text\n",
    "file_obj = open(\"output.txt\", \"r\", encoding=\"utf-8\")\n",
    "print(file_obj.read(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1151d28e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cleaned_strg1 = ''.join(map(str,cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9636f10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353781724"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_strg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637acf33",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sahluwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sahluwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#This step is lemmatizing the text which standardizes words by reducing to base form for better analysis.\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "       \n",
    "def lemmatize_passage(cleaned_strg1):\n",
    "    words = word_tokenize(cleaned_strg1)\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "result = lemmatize_passage(cleaned_strg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7db0177",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8720ed05-6db7-42e3-a008-d0a78d5811e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully as 'result.txt'\n"
     ]
    }
   ],
   "source": [
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "     for item in result:         \n",
    "         f.write(item)\n",
    "print(\"File saved successfully as 'result.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bcd623b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#The next step is vectorizing the text and we need to change the string to a list.\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "list_result = Convert(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28b554f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c7068d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78ed56e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#LDA topic model algorithm requires a document word matrix, which is what we are creating here.\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             #stop_words='english',             # remove stop words\n",
    "                             #lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n",
    "                             # max_features=50000,             # max number of unique words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(list_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54452941",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (5, 351)\t1\n",
      "  (6, 348)\t1\n",
      "  (7, 1290)\t1\n",
      "  (8, 1078)\t1\n",
      "  (9, 475)\t1\n"
     ]
    }
   ],
   "source": [
    "print(data_vectorized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fee8289b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vector = data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2ad3f62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42595164, 1416)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "322f022b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 88)\t1\n",
      "  (1, 957)\t1\n",
      "  (2, 417)\t1\n",
      "  (3, 1089)\t1\n",
      "  (5, 668)\t1\n",
      "  (6, 930)\t1\n",
      "  (7, 1340)\t1\n",
      "  (8, 1257)\t1\n",
      "  (9, 979)\t1\n"
     ]
    }
   ],
   "source": [
    "#This code selects random rows fom the vector because the size of the vector may be too large \n",
    "#for available computing capacity.\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def select_random_rows(matrix, num_rows):\n",
    "    # Get the number of rows in the matrix\n",
    "    total_rows = matrix.shape[0]\n",
    "    \n",
    "    # Generate random row indices\n",
    "    random_indices = np.random.choice(total_rows, size=num_rows, replace=False)\n",
    "    \n",
    "    # Extract the selected rows\n",
    "    selected_rows = matrix[random_indices]\n",
    "    \n",
    "    # Create a new csr_matrix from the selected rows\n",
    "    new_matrix = csr_matrix(selected_rows)\n",
    "    \n",
    "    return new_matrix\n",
    "\n",
    "num_rows_to_select = 1000000\n",
    "new_matrix = select_random_rows(vector, num_rows_to_select)\n",
    "\n",
    "print(new_matrix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fce73b7-4a05-4884-b984-083c4ab0b64e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1416)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c23022",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sparsicity is the percentage of non-zero datapoints in the document-word matrix, that is data_vectorized.\n",
    "# Since most cells in this matrix will be zero, I am interested in knowing what percentage of cells --\n",
    "# contain non-zero values.\n",
    "data_dense = new_matrix.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e66497b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools if you decide to create visuals\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.sklearn\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import spacy\n",
    "#import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f118044b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=200, learning_method='online', max_iter=5,\n",
      "                          n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10,            # Number of topics\n",
    "                                      max_iter=5,                # Max learning iterations\n",
    "                                      learning_method='online',  # The method used for learning 'online' indicates online variational Bayes\n",
    "                                      #random_state=100,          # Random state When you set a specific value for random_state, you guarantee that the same data points will be included in the training and testing sets every time you run the code.\n",
    "                                      batch_size=200,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: -1 skips the perplexity\n",
    "                                      n_jobs = -1,               # Use all available CPUs; -1 uses all available cores\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(new_matrix)\n",
    "\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e18d5e93",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -6392977.39616302\n",
      "Perplexity:  693.8334000611763\n",
      "{'batch_size': 200,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 5,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': None,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(new_matrix))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(new_matrix))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c489cc1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
       "                         &#x27;n_components&#x27;: [5, 8, 12, 16]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
       "                         &#x27;n_components&#x27;: [5, 8, 12, 16]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: LatentDirichletAllocation</label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(learning_decay=0.9, n_components=5)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LatentDirichletAllocation<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(learning_decay=0.9, n_components=5)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [5, 8, 12, 16]})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we are creating search parameters to see which ones work best for the model\n",
    "search_params = {'n_components': [5, 8, 12, 16], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01712fbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(error_score='raise',\n",
       "             estimator=LatentDirichletAllocation(batch_size=100,\n",
       "                                                 learning_method=None,\n",
       "                                                 n_components=7, n_jobs=1),\n",
       "             n_jobs=1,\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_topics': [5, 8, 12, 16]},\n",
       "             return_train_score='warn')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can skip this or use depending on if you want to use the method above.\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=100, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=7, n_jobs=1,\n",
    "             perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "       n_jobs=1,\n",
    "       param_grid={'n_topics': [5, 8, 12, 16], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a13e9a9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -1288081.7278126136\n",
      "Model Perplexity:  693.2089881828424\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(new_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb0c7a41",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>105</th>\n",
       "      <th>1115</th>\n",
       "      <th>1172</th>\n",
       "      <th>2009</th>\n",
       "      <th>2021</th>\n",
       "      <th>2023</th>\n",
       "      <th>2025</th>\n",
       "      <th>2050</th>\n",
       "      <th>21st</th>\n",
       "      <th>5axi</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwid</th>\n",
       "      <th>worthi</th>\n",
       "      <th>wsu</th>\n",
       "      <th>wvsu</th>\n",
       "      <th>wvu</th>\n",
       "      <th>wyom</th>\n",
       "      <th>x9d</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>0.200034</td>\n",
       "      <td>142.199871</td>\n",
       "      <td>174.199871</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>1237.199871</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200018</td>\n",
       "      <td>464.199871</td>\n",
       "      <td>0.200034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>173.199872</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>623.199872</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>4372.199872</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200018</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>1411.199872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>299.199847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>294.199847</td>\n",
       "      <td>0.200855</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>142.199851</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>349.199851</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>160.199851</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>295.199851</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200016</td>\n",
       "      <td>0.200030</td>\n",
       "      <td>0.200030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>144.199874</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>316.199874</td>\n",
       "      <td>143.199874</td>\n",
       "      <td>0.200040</td>\n",
       "      <td>...</td>\n",
       "      <td>561.199874</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>156.199874</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200040</td>\n",
       "      <td>811.199092</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                105        1115        1172        2009        2021  \\\n",
       "Topic 0    0.200034  142.199871  174.199871    0.200038    0.200034   \n",
       "Topic 1  173.199872    0.200034    0.200034    0.200039    0.200034   \n",
       "Topic 2    0.200029    0.200030    0.200030    0.200033    0.200029   \n",
       "Topic 3    0.200030    0.200030    0.200030  142.199851    0.200030   \n",
       "Topic 4    0.200034    0.200035    0.200035    0.200039  144.199874   \n",
       "\n",
       "               2023        2025        2050        21st        5axi  ...  \\\n",
       "Topic 0    0.200038    0.200034    0.200033    0.200034    0.200039  ...   \n",
       "Topic 1    0.200039  623.199872    0.200034    0.200034    0.200039  ...   \n",
       "Topic 2    0.200033    0.200029    0.200029    0.200029  299.199847  ...   \n",
       "Topic 3  349.199851    0.200030    0.200030    0.200030    0.200035  ...   \n",
       "Topic 4    0.200039    0.200034  316.199874  143.199874    0.200040  ...   \n",
       "\n",
       "              world    worldwid       worthi         wsu         wvsu  \\\n",
       "Topic 0    0.200033    0.200038     0.200034    0.200033  1237.199871   \n",
       "Topic 1    0.200034    0.200039  4372.199872    0.200034     0.200034   \n",
       "Topic 2    0.200029    0.200033     0.200029    0.200029     0.200030   \n",
       "Topic 3    0.200030  160.199851     0.200030    0.200030     0.200030   \n",
       "Topic 4  561.199874    0.200039     0.200034  156.199874     0.200035   \n",
       "\n",
       "                wvu        wyom         x9d        year        yield  \n",
       "Topic 0    0.200038    0.200039    0.200018  464.199871     0.200034  \n",
       "Topic 1    0.200039    0.200039    0.200018    0.200034  1411.199872  \n",
       "Topic 2    0.200033  294.199847    0.200855    0.200030     0.200029  \n",
       "Topic 3  295.199851    0.200035    0.200016    0.200030     0.200030  \n",
       "Topic 4    0.200039    0.200040  811.199092    0.200035     0.200034  \n",
       "\n",
       "[5 rows x 1416 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "topicnames = [f'Topic {i}' for i in range(best_lda_model.n_components)]\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names_out()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf101ed2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_topic_keywords.to_csv('LDAtopics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67b3487f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>develop</td>\n",
       "      <td>student</td>\n",
       "      <td>institut</td>\n",
       "      <td>food</td>\n",
       "      <td>applic</td>\n",
       "      <td>plant</td>\n",
       "      <td>review</td>\n",
       "      <td>includ</td>\n",
       "      <td>method</td>\n",
       "      <td>graduat</td>\n",
       "      <td>region</td>\n",
       "      <td>precis</td>\n",
       "      <td>interdisciplinari</td>\n",
       "      <td>plan</td>\n",
       "      <td>particip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>learn</td>\n",
       "      <td>advanc</td>\n",
       "      <td>foundat</td>\n",
       "      <td>intellectu</td>\n",
       "      <td>worthi</td>\n",
       "      <td>design</td>\n",
       "      <td>educ</td>\n",
       "      <td>comput</td>\n",
       "      <td>energi</td>\n",
       "      <td>rural</td>\n",
       "      <td>area</td>\n",
       "      <td>manufactur</td>\n",
       "      <td>improv</td>\n",
       "      <td>structur</td>\n",
       "      <td>explor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>univers</td>\n",
       "      <td>impact</td>\n",
       "      <td>challeng</td>\n",
       "      <td>broad</td>\n",
       "      <td>sustain</td>\n",
       "      <td>train</td>\n",
       "      <td>innov</td>\n",
       "      <td>model</td>\n",
       "      <td>mission</td>\n",
       "      <td>deem</td>\n",
       "      <td>nsfs</td>\n",
       "      <td>domain</td>\n",
       "      <td>collabor</td>\n",
       "      <td>state</td>\n",
       "      <td>involv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>research</td>\n",
       "      <td>data</td>\n",
       "      <td>scienc</td>\n",
       "      <td>evalu</td>\n",
       "      <td>program</td>\n",
       "      <td>industri</td>\n",
       "      <td>reflect</td>\n",
       "      <td>merit</td>\n",
       "      <td>need</td>\n",
       "      <td>criterion</td>\n",
       "      <td>workforc</td>\n",
       "      <td>farm</td>\n",
       "      <td>machin</td>\n",
       "      <td>network</td>\n",
       "      <td>isac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>technolog</td>\n",
       "      <td>crop</td>\n",
       "      <td>infrastructur</td>\n",
       "      <td>integr</td>\n",
       "      <td>solut</td>\n",
       "      <td>approach</td>\n",
       "      <td>problem</td>\n",
       "      <td>capabl</td>\n",
       "      <td>enabl</td>\n",
       "      <td>propos</td>\n",
       "      <td>aim</td>\n",
       "      <td>partnership</td>\n",
       "      <td>associ</td>\n",
       "      <td>land</td>\n",
       "      <td>manag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word 0   Word 1         Word 2      Word 3   Word 4    Word 5  \\\n",
       "Topic 0    develop  student       institut        food   applic     plant   \n",
       "Topic 1      learn   advanc        foundat  intellectu   worthi    design   \n",
       "Topic 2    univers   impact       challeng       broad  sustain     train   \n",
       "Topic 3   research     data         scienc       evalu  program  industri   \n",
       "Topic 4  technolog     crop  infrastructur      integr    solut  approach   \n",
       "\n",
       "          Word 6  Word 7   Word 8     Word 9   Word 10      Word 11  \\\n",
       "Topic 0   review  includ   method    graduat    region       precis   \n",
       "Topic 1     educ  comput   energi      rural      area   manufactur   \n",
       "Topic 2    innov   model  mission       deem      nsfs       domain   \n",
       "Topic 3  reflect   merit     need  criterion  workforc         farm   \n",
       "Topic 4  problem  capabl    enabl     propos       aim  partnership   \n",
       "\n",
       "                   Word 12   Word 13   Word 14  \n",
       "Topic 0  interdisciplinari      plan  particip  \n",
       "Topic 1             improv  structur    explor  \n",
       "Topic 2           collabor     state    involv  \n",
       "Topic 3             machin   network      isac  \n",
       "Topic 4             associ      land     manag  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names_out())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de9b0a3a-9612-4747-9852-d7432d56f179",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the necessary components\n",
    "joblib.dump(best_lda_model, 'lda_model.pkl')  # Save LDA model\n",
    "joblib.dump(new_matrix, 'new_matrix.pkl')    # Save document-term matrix\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')    # Save vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76883b-980d-4dd5-9252-99af15572f46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
