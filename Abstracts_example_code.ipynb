{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01bf7e9-07f9-4b7e-9fd7-a9359b063025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efdad8-6e23-4f0f-a75a-bbd52cd4a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('textfile.xlsx')\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34c0fd-443b-482b-a5e7-d95379cacbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_series = df.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "combined_text = ' '.join(merged_series)\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa18bf-3a3c-45bd-9885-12c8cf154f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = str(combined_text)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd0a97-89a2-43ee-a658-563dd65c42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_add= ['from', 'subject', 're', 'edu', 'use','also', 'well', 'including', 'simple', 'yet',  'work', 'inform',  'generally','given', 'work', 'help', 'play', \n",
    "                'et', 'al','across', 'towards','allow', 'body','affect','give', 'addtion','know', 'whose','real','open','often','although','act','find','even',\n",
    "              'suggest','connect','tightly','release','space','function','regulate','control','role','apt','process', 'finely', 'tune','regulation','critical','functioning',\n",
    "              'endeavor','finely','conditions','term','project','using','changes','fundamental', 'build','block','make','addition', 'provide', 'train','individual',\n",
    "              'feasible', 'complex','mechanisms', 'remain', 'unclear', 'overarch','system', 'thereby', 'movement', 'valuable', 'educational','consider','waste',\n",
    "              'study', 'field', 'establish','award','understand','expression','community','reach', 'the', 'on', 'in',\n",
    "              'statutory','statutory','head','call','group','new','size','behavior','leave','internalize',\n",
    "              'via','ecosystem','model','organisms','harsh','concurrently','precise','small','essential','long','release',\n",
    "               'random','live','range','really','form','global','potential','collection','important','exposure','extreame','order', 'recipient','extreme','break','still','together',\n",
    "              'cells','build','block', 'live','however','dont', 'exactly', 'cells', 'form', 'first', 'place', 'dont','molecules','cell', 'finally','question', 'challenge', 'pose', 'require',...]\n",
    "stop_words.update(stopwords_add)\n",
    "# stopwords.extend(stopwords_add)\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c802aa-c63c-4582-bae9-0d2a8dd1d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(combined_text):\n",
    "    import string\n",
    "    # nltk.download('punkt')  # Download tokenizer if not already done\n",
    "    text = combined_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    tokens = text.split()\n",
    "    # tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "filtered_text = remove_stopwords(combined_text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5841d1-3aa4-4a49-a79a-ffba1f712b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer_w = WhitespaceTokenizer()\n",
    "tokenized_text = tokenizer_w.tokenize(filtered_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c9112-39e1-4d07-9edb-bd5ef7f40ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658c020-6345-43c5-b8e4-6f2bcae10792",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_clean_text = [re.sub(\"\\'\", \"\", str(tokenized_text)) for sent in tokenized_text]\n",
    "print(token_clean_text[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dca2af-cb0b-489e-a19d-83b3267b2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_clean_strg = str(token_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893aa3f3-fd00-421c-8800-6328c1258a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download ('punkt_tab')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", True)\n",
    "words = word_tokenize(token_clean_strg)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f1e6f-5369-4b91-9f23-6c1a63d5ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words_strg = str(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001e845-bf96-4018-91fc-add71fedb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [sent.replace(\"'\", \"\").replace(\",\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\") for sent in stemmed_words_strg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a5028-a9d5-4da5-a6af-a809fb993ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "     for item in cleaned_texts:         \n",
    "         f.write(item)\n",
    "print(\"File saved successfully as 'output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d205047-1e02-4330-938c-9a4c352d83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = open(\"output.txt\", \"r\", encoding=\"utf-8\")\n",
    "print(file_obj.read(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc723c0-49d8-4173-bb0f-b839b77aa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_strg1 = ''.join(map(str,cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f718fbe-314d-42bb-8035-57aa7d6780b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "       \n",
    "def lemmatize_passage(cleaned_strg1):\n",
    "    words = word_tokenize(cleaned_strg1)\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "result = lemmatize_passage(cleaned_strg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a486da-e2e0-402c-a750-007b30cb6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "     for item in result:         \n",
    "         f.write(item)\n",
    "print(\"File saved successfully as 'result.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97fe1d-7771-4f7a-9f0a-d46b88942810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "list_result = Convert(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ecb98-3654-454b-be3e-c4fb46c8ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             #stop_words='english',             # remove stop words\n",
    "                             #lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n",
    "                             # max_features=50000,             # max number of unique words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(list_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cba54-824f-4aff-ae7b-edff03da97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51513ce-b487-4dc1-9013-9490292d5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def select_random_rows(matrix, num_rows):\n",
    "    # Get the number of rows in the matrix\n",
    "    total_rows = matrix.shape[0]\n",
    "    \n",
    "    # Generate random row indices\n",
    "    random_indices = np.random.choice(total_rows, size=num_rows, replace=False)\n",
    "    \n",
    "    # Extract the selected rows\n",
    "    selected_rows = matrix[random_indices]\n",
    "    \n",
    "    # Create a new csr_matrix from the selected rows\n",
    "    new_matrix = csr_matrix(selected_rows)\n",
    "    \n",
    "    return new_matrix\n",
    "\n",
    "num_rows_to_select = 1000000\n",
    "new_matrix = select_random_rows(vector, num_rows_to_select)\n",
    "\n",
    "print(new_matrix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8506b2c-2a1d-427d-8b14-5c30242a67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense = new_matrix.todense()\n",
    "print(\"Sparsity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f77c9-5279-4edb-a8b2-185bfb33abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b62a25-7923-46e5-924f-edbc4ce010a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10,            # Number of topics\n",
    "                                      max_iter=5,                # Max learning iterations\n",
    "                                      learning_method='online',  # The method used for learning 'online' indicates online variational Bayes\n",
    "                                      #random_state=100,          # Random state When you set a specific value for random_state, you guarantee that the same data points will be included in the training and testing sets every time you run the code.\n",
    "                                      batch_size=200,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: -1 skips the perplexity\n",
    "                                      n_jobs = -1,               # Use all available CPUs; -1 uses all available cores\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(new_matrix)\n",
    "\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2c6f8-a434-4322-9fe9-b2f57008b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Log Likelihood: \", lda_model.score(new_matrix))\n",
    "print(\"Perplexity: \", lda_model.perplexity(new_matrix))\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08f4b7-91ae-44a7-95f5-fab072e43884",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'n_components': [5, 8, 12, 16], 'learning_decay': [.5, .7, .9]}\n",
    "lda = LatentDirichletAllocation()\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb49db-acc7-44c8-888f-e8085ed24731",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(new_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490df91c-ccb8-45ba-b6d3-304f2cf8d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "topicnames = [f'Topic {i}' for i in range(best_lda_model.n_components)]\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names_out()\n",
    "df_topic_keywords.index = topicnames\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549127bd-f4cf-481b-a84b-275f3958f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_keywords.to_csv('LDAtopics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d600d5d-db55-4d6c-a1aa-3b38df9fae41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
